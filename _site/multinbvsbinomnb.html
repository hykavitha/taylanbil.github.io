<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=6cdbfa69a0d8ea7fefd9ebfecd09834b2930fd0c">

  </head>

  <body>

    <header>
      <div class="container">
        <img src="/img/nation.png" style="float: right; width: 20%; margin-right: 1%; margin-bottom: 1%;" border="5"/>
        <h1>taylanbil.github.io:wq!</h1>
        <h2>Math, Data, Python, Machine Learning @ Facebook</h2>

        <section id="downloads">
          
          <a href="http://github.com/taylanbil/taylanbil.github.io" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
          <a href="/" class="btn btn-home"><span></span>Home</a>
        </section>
      </div>
    </header>

    <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <div class="container">
      <section id="main_content">
        <h1 id="a-close-look-to-sklearns-naive-bayes-nb-algorithms">A close look to Sklearn’s Naive Bayes (NB) algorithms</h1>

<h2 id="intro">Intro:</h2>

<p>Naive Bayes (NB) is a popular algorithm for text-related classification tasks. Sklearn’s Multinomial and Bernoulli NB implementations have subtle but quite significant nuances/differences between them. This post will look into these nuances, try to clarify assumptions behind each type of Naive Bayes and try to explain when to use each one.</p>

<p><code class="highlighter-rouge">sklearn</code>’s MultinomialNB and BernoulliNB implementations make certain silent assumptions on what kind of datasets they are being trained on. To use these ML algorithms properly, one needs to prepare the training datasets in accordance with these assumptions, which is often overlooked by ML practitioners and this consequently leads to models that are improperly trained.</p>

<hr />

<p>Let’s first see how they behave differently in a very simple example. In the following toy example, our training set is binary, and very small. Let’s jump right in:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># First, some imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'x1'</span><span class="p">,</span> <span class="s">'x2'</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="n">X</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'y'</span><span class="p">))</span>
</code></pre>
</div>

<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>So, as you see, we have 5 training samples, with two features. The first two samples belong to class 0, and the other 3 belong to class 1.</p>

<p>Let’s fit a <code class="highlighter-rouge">Multinomial</code> and <code class="highlighter-rouge">Bernoulli</code> Naive Bayes classifier to this toy dataset. After the models are fit, let’s apply it on a sample and see what happens.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span><span class="p">,</span> <span class="n">BernoulliNB</span>

<span class="c"># no smoothing here, in order to easily understand the computations under the hood</span>
<span class="n">mnb</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  
<span class="n">bnb</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">bnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</code></pre>
</div>

<p>The models are fit now. Let’s apply it to a sample, where the features <code class="highlighter-rouge">x1</code> and <code class="highlighter-rouge">x2</code> are both 1.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">new_sample</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>


<span class="k">print</span><span class="p">(</span><span class="n">mnb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">new</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">bnb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">new</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>[[ 0.4  0.6]]
[[ 0.6  0.4]]
</code></pre>
</div>

<p>They predict complete opposite probabilities! Ok, what is happening?</p>

<p>Let’s compute by hand:</p>

<h3 id="binomial"><strong>Binomial</strong>:</h3>

<p>The way we think here is as follows:</p>

<p>Each row is a document.<br />
Each feature is a flag, representing <strong>if a word exist in that document or not</strong>.
Thus, we end up computing the following conditional probabilities (likelihoods).</p>

<ul>
  <li>P(<span style="color:green"><strong>a document has x1 in it</strong></span> | document is 0) = 1/2<br />
    <em>Got this from counting the samples where x1 is 1, and class is 0</em></li>
  <li>P(<span style="color:green"><strong>a document has x2 in it</strong></span> | document is 0) = 1/2<br />
    <em>Similarly, count samples where x1 is 1, and class is 0</em></li>
  <li>P(document is 0) = 2/5<br />
    <em>Got this from counting 0s in the y array</em></li>
</ul>

<p>So, P(new is 0) ~= 1/2.1/2.2/5 = 1/10.</p>

<p>Similarly,</p>

<ul>
  <li>P(<span style="color:green"><strong>a document has x1 in it</strong></span> | document is 1) = 1/3<br />
    <em>Got this from counting the samples where x1 is 1, and class is 1</em></li>
  <li>P(<span style="color:green"><strong>a document has x2 in it</strong></span> | document is 1) = 1/3 <br />
    <em>Similarly, count samples where x1 is 1, and class is 1</em></li>
  <li>P(document is 1) = 3/5<br />
    <em>Got this from counting 1s in the y array</em></li>
</ul>

<p>So, P(new is 1) ~= 1/3.1/3.3/5 = 1/15</p>

<p>So, P(new=0) is 0.6 and P(new=1) = 0.4  (normalized the probas 1/10 and 1/15 here)</p>

<hr />

<h3 id="multinomial"><strong>Multinomial</strong>:</h3>

<p>The way we think here is different.</p>

<p>Each row is still a document.
Each column is now a count, of a word in the documents. This is why MultinomialNB can work with matrices whose entries are non-binary, but positive.</p>

<p>Thus, we end up computing the following conditional probabilities (likelihoods).</p>

<ul>
  <li>P(<span style="color:orange"><strong>given word is x1</strong></span> | document is 0) = 1/2<br />
    <em>This is because documents labeled 0 have only 2 words in them, and 1 of them is x1.</em></li>
  <li>P(<span style="color:orange"><strong>given word is x2</strong></span> | document is 0) = 1/2<br />
    <em>Same.</em></li>
  <li>P(document is 0) = 2/5<br />
    <em>Got this from counting 0s in the y array</em></li>
</ul>

<p>So far it is the same, but it differs in below:</p>

<ul>
  <li>P(<span style="color:orange"><strong>a document has x1 in it</strong></span> | document is 1) = 1/2<br />
    <span style="color:red"><strong><em>Although there are 3 documents labeled 1, there are 2 words in doc1, and only 1 of them is x1</em></strong></span></li>
  <li>P(<span style="color:orange"><strong>a document has x2 in it</strong></span> | document is 1) = 1/2<br />
    <em>Same.</em></li>
  <li>P(document is 0) = 3/5<br />
    <em>Got this from counting 1s in the y array</em></li>
</ul>

<p>Now, work out the probas, and you’ll find</p>

<p>P(new=0) is 0.4 and P(new=1) = 0.6.</p>

<hr />

<p>The difference lies in the likelihoods we’re computing. In multinomial, we’re calculating horizontally (so to speak), the denominator has data from other columns. In binomial, we’re calculating vertically only, the denominator does not see any of the other columns.</p>

<hr />

<p><strong>IMPORTANT</strong>: So with these concepts in mind; it should be clear that using <code class="highlighter-rouge">MultinomialNB</code> with an input matrix containing a negative number does not make sense. In fact, it will just error out if you try to do so. On the other hand, <code class="highlighter-rouge">BernoulliNB</code> technically works with continuous data too, because it first <strong>binarizes it (with default threshold 0)</strong>. Trying these algorithms on a general dataset without taking measures against these things is will lead to meaningless models in practice.</p>

<h1 id="conclusion">Conclusion:</h1>

<p><code class="highlighter-rouge">sklearn</code>’s <code class="highlighter-rouge">MultinomialNB</code> and <code class="highlighter-rouge">BernoulliNB</code> are implementations of Naive Bayes that are built with NLP-related tasks in mind, especially <code class="highlighter-rouge">MultinomialNB</code>. Specifically, they assume the input they receive are coming from a text preprocessor such as <code class="highlighter-rouge">CountVectorizer</code> or <code class="highlighter-rouge">TfidfVectorizer</code>. One needs to do a lot of cleanup and preparation in order to use it in a more general setting, taking how these algorithms interpret the input data into consideration. In the next post, I will explore a more general implementation of Naive Bayes which will apply outside the NLP related tasks.</p>

      </section>
    </div>

    
      <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-106417291-1', 'auto');
        ga('send', 'pageview');
      </script>
    
  </body>
</html>
