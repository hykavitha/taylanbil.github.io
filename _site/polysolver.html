<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/css/style.css?v=6cdbfa69a0d8ea7fefd9ebfecd09834b2930fd0c">

  </head>

  <body>

    <header>
      <div class="container">
        <img src="/img/nation.png" style="float: right; width: 20%; margin-right: 1%; margin-bottom: 1%;" border="5"/>
        <h1>taylanbil.github.io:wq!</h1>
        <h2>Math, Data, Python, Machine Learning @ Facebook</h2>

        <section id="downloads">
          
          <a href="http://github.com/taylanbil/taylanbil.github.io" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
          <a href="/" class="btn btn-home"><span></span>Home</a>
        </section>
      </div>
    </header>

    <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

    <div class="container">
      <section id="main_content">
        <h1 id="a-neural-network-that-solves-polynomials">A neural network that solves polynomials</h1>

<h2 id="0-introduction">0. Introduction</h2>

<p>This post is based on <a href="https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py">this</a> wonderful example of a neural network that learns to add two given numbers. In the keras example, the inputs are numbers, however the network sees them as encoded characters. So, in effect, the network has no awareness of the inputs, specifically of their ordinal nature. And magically, it still learns to add the two input sequences (of numbers, which it sees as characters), and spits out the correct answer more often than not.</p>

<p>Here, my goal is to build on this (non-useful but cool) idea of formulating a math problem as a machine learning problem, and code up a Neural Network which learns to solve polynomials.</p>

<h2 id="1-formulating-the-problem">1. Formulating the problem</h2>

<p>A polynomial is a mathematical function of the form:</p>

<script type="math/tex; mode=display">f(x) = a_nx^n + \ldots + a_2x^2 + a_1x + a_0</script>

<p>There are no guarantees that such a function will have a root, i.e. there may be no \(x\) values for which \(f(x) = 0\). For our purposes, I will create a set of such polynomials for which there are roots. In fact, I will first create the roots, and then create the polynomials, as follows:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># First, let's make the dataset:</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">MIN_ROOT</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">MAX_ROOT</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">make</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_degree</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">MAX_ROOT</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">MAX_ROOT</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_degree</span><span class="p">))</span>
    <span class="n">y</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">poly</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">y</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="c"># toy case</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>[[ 1.         -0.22689484 -0.00127547]]
[[-0.00548864  0.23238348]]
</code></pre>
</div>

<p>Here in the toy example, I have a , we draw two random numbers between -1 and 1. The numbers drawn are  -0.00548864 and 0.23238348. Consequently, the polynomial that has those roots is produced by <code class="highlighter-rouge">numpy</code>’s <code class="highlighter-rouge">poly</code> function. Specifically,</p>

<script type="math/tex; mode=display">f(x) = x^2  -0.22689484x -0.00127547</script>

<p>becomes zero for \(x = -0.00548864\) and \(x = 0.23238348\). Note that because we selected 2 numbers the degree of the polynomial is 2 as well.</p>

<hr />

<p>What I want to focus on in this toy dataset is, that a polynomial is represented as an ordered sequence of real numbers. It starts from the highest coefficient, and then the next coefficient, and then the next, and so on.. Not only the coefficients, but also the roots are represented as a sequence. They are sorted random numbers between -1 and 1. So, this is a <code class="highlighter-rouge">seq2seq</code> problem! <code class="highlighter-rouge">seq2seq</code> refers to the deep learning structures that take a sequence of things (usually words) as input, and spits out a sequence of things (usually the same “thing” as the input “thing”).</p>

<p><img src="https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67" alt="" /></p>

<p>Generally, the use cases for <code class="highlighter-rouge">seq2seq</code> are NLP focused, such as question answering, translation between 2 languages and so on. In this example, our input sequence will be polynomial coefficients, and the output sequence will be the roots, in sorted order.</p>

<p>Note that there are pretty good numerical methods solving polynomials with given coefficients, which uses the companion matrix. See <code class="highlighter-rouge">numpy</code>’s <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.roots.html#numpy.roots">roots</a> function.</p>

<h2 id="2-lets-go">2. Let’s go!</h2>

<p>It is well known that polynomials of degree \(&gt;=5\) are not solvable by radicals. This was mathematically proved using Galois theory a long time ago. So, let’s have our first network (numerically and approximately and probabilistically :) ) solve degree 5 polynomials.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># make and train test split</span>
<span class="n">N_SAMPLES</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="n">DEGREE</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">make</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N_SAMPLES</span><span class="o">*</span><span class="mf">0.8</span><span class="p">),</span> <span class="n">DEGREE</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">make</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N_SAMPLES</span><span class="o">*</span><span class="mf">0.2</span><span class="p">),</span> <span class="n">DEGREE</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(80000, 6) (80000, 5)
(20000, 6) (20000, 5)
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># some dimensionality handling below</span>
<span class="c"># this is because keras expects 3d tensors (a seq of onehot encoded characters) </span>
<span class="c"># but we have 2d tensors; just a sequence of numbers.</span>


<span class="k">def</span> <span class="nf">reshape</span><span class="p">(</span><span class="n">array</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    
<span class="k">print</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c"># batchsize, timesteps, input_dim (or "vocab_size")</span>
<span class="k">print</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>(20000, 6, 1)
(20000, 5, 1)
</code></pre>
</div>

<p>Now that the dataset is ready, let’s create the model. We will have two recurrent layers, one as the encoder part and the other as the decoder part (see image above). I will use LSTM for the recurrent layers as it performs well more often than not. Note that this is seq2seq with un-shared weights.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">RepeatVector</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">TimeDistributed</span>


<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c"># ENCODER PART OF SEQ2SEQ</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">DEGREE</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="c"># DECODER PART OF SEQ2SEQ</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span class="p">(</span><span class="n">DEGREE</span><span class="p">))</span>  <span class="c"># this determines the length of the output sequence</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">LSTM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_absolute_error'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Using TensorFlow backend.


_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 128)               66560     
_________________________________________________________________
repeat_vector_1 (RepeatVecto (None, 5, 128)            0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 5, 128)            131584    
_________________________________________________________________
time_distributed_1 (TimeDist (None, 5, 1)              129       
=================================================================
Total params: 198,273
Trainable params: 198,273
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
          <span class="n">reshape</span><span class="p">(</span><span class="n">y_train</span><span class="p">),</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
          <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> 
                           <span class="n">reshape</span><span class="p">(</span><span class="n">y_test</span><span class="p">)))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Train on 80000 samples, validate on 20000 samples
Epoch 1/5
80000/80000 [==============================] - 19s 239us/step - loss: 0.1538 - mean_absolute_error: 0.1538 - val_loss: 0.1109 - val_mean_absolute_error: 0.1109
Epoch 2/5
80000/80000 [==============================] - 19s 232us/step - loss: 0.1066 - mean_absolute_error: 0.1066 - val_loss: 0.1064 - val_mean_absolute_error: 0.1064
Epoch 3/5
80000/80000 [==============================] - 18s 230us/step - loss: 0.0916 - mean_absolute_error: 0.0916 - val_loss: 0.0879 - val_mean_absolute_error: 0.0879
Epoch 4/5
80000/80000 [==============================] - 18s 230us/step - loss: 0.0788 - mean_absolute_error: 0.0788 - val_loss: 0.0794 - val_mean_absolute_error: 0.0794
Epoch 5/5
80000/80000 [==============================] - 18s 229us/step - loss: 0.0732 - mean_absolute_error: 0.0732 - val_loss: 0.0703 - val_mean_absolute_error: 0.0703





&lt;keras.callbacks.History at 0x7f80ff57fb70&gt;
</code></pre>
</div>

<p>Mean Absolute Error is going down… that’s great.<br />
This problem is a bit difficult to evaluate. Actually, so is almost all seq2seq problems.</p>

<p>I am not aware of any standard way to evaluate the success of this model, so I got creative and thought of two ways of evaluating such a problem.</p>

<ol>
  <li>Comparing the predicted roots to random guessing; i.e. for a given polynomial, we’ll generate random numbers between -1 and 1 (as many as the DEGREE), sort them and look at the difference in absolute value between each pair (random, true root). We’ll then do the same thing with each pair (predicted root, true root). Then we’ll plot the histogram for such differences.</li>
  <li>We’ll generate random numbers as above, and evaluate the polynomial. We’ll then evaluate the polynomial on the predicted roots. Then we’ll do these evaluations for each polynomial and plot the histogram.</li>
</ol>

<p>First, get the predictions and random roots.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</code></pre>
</div>

<p>Now a function to help us compare, that we can re-use</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>


<span class="k">def</span> <span class="nf">get_evals</span><span class="p">(</span><span class="n">polynomials</span><span class="p">,</span> <span class="n">roots</span><span class="p">):</span>
    <span class="n">evals</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">poly</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">root_row</span><span class="p">]</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">root_row</span><span class="p">,</span> <span class="n">poly</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="n">polynomials</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">evals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">evals</span>
    

<span class="k">def</span> <span class="nf">compare_to_random</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">polynomials</span><span class="p">):</span>
    <span class="n">y_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">MAX_ROOT</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">y_random</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">y_random</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> 
            <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'random guessing'</span><span class="p">)</span> 
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">((</span><span class="n">y_pred</span><span class="o">-</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()),</span> 
            <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'model predictions'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'Histogram of absolute errors'</span><span class="p">,</span>
           <span class="n">ylabel</span><span class="o">=</span><span class="s">'count'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">'absolute error'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>

    <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">random_evals</span> <span class="o">=</span> <span class="n">get_evals</span><span class="p">(</span><span class="n">polynomials</span><span class="p">,</span> <span class="n">y_random</span><span class="p">)</span>
    <span class="n">predicted_evals</span> <span class="o">=</span> <span class="n">get_evals</span><span class="p">(</span><span class="n">polynomials</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">random_evals</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'random guessing kde'</span><span class="p">)</span>
    <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predicted_evals</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'model prediction kde'</span><span class="p">)</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s">'Kernel Density Estimate plot</span><span class="se">\n</span><span class="s">'</span> \
            <span class="s">'for polynomial evaluation of (predicted) roots'</span>
    <span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre>
</div>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">compare_to_random</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</code></pre>
</div>

<p><img src="img/PolynomialSolver_files/PolynomialSolver_12_0.png" alt="png" /></p>

<p>These results look good to me! And this is after just 5 epochs!</p>

<p>On the left plot, notice how the predicted roots (red bars) are closer to the real roots (i.e. red bars are small after 0.25). On the right, notice how tight the red curve is around zero. That is, the expected distribution of the evaluation of polynomials at the model predicted roots is very tightly contained in the neighborhood of zero. At least there’s a stark difference between that and e random evaluations.</p>

<hr />

<p>Now I want to do something a bit more ambitious. I would like to modify the example above by removing the condition of solving degree 5 polynomials.</p>

<h2 id="3-solving-polynomials-of-varying-degree">3. Solving polynomials of varying degree</h2>

<p>This next example will solve polynomials of degree greater than 4 and less than 16. This is necessary since in the numerical techniques we employ, we need to give the network a maximum sequence length among other things.</p>

<p>First, let’s create our datasets. We’ll have 10k examples per degree. A couple of things to note:</p>

<ul>
  <li>Every polynomial will be represented as a sequence of length 16. If for a given polynomial, the degree is less than 15, the sequence will be padded by zeros.</li>
  <li>The roots will be represented as TWO sequences of length 15. Specifically, it will be a 15x2 matrix. The first column will be the actual values of the roots, padded by 0. The second column is whether the root value was padded or not. Think of it as one-hot-encoding the EOS (end-of-sequence) character.</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">MAX_DEGREE</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">MIN_DEGREE</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">MAX_ROOT</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">MIN_ROOT</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">N_SAMPLES</span> <span class="o">=</span> <span class="mi">10000</span> <span class="o">*</span> <span class="p">(</span><span class="n">MAX_DEGREE</span><span class="o">-</span><span class="n">MIN_DEGREE</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">make</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">max_degree</span><span class="p">,</span> <span class="n">min_degree</span><span class="p">,</span> <span class="n">min_root</span><span class="p">,</span> <span class="n">max_root</span><span class="p">):</span>
    <span class="n">samples_per_degree</span> <span class="o">=</span> <span class="n">n_samples</span> <span class="o">//</span> <span class="p">(</span><span class="n">max_degree</span><span class="o">-</span><span class="n">min_degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">samples_per_degree</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_degree</span><span class="o">-</span><span class="n">min_degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">max_degree</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="c"># XXX: filling the truth labels with ZERO??? EOS character would be nice</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">max_degree</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">degree</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">min_degree</span><span class="p">,</span> <span class="n">max_degree</span><span class="o">+</span><span class="mi">1</span><span class="p">)):</span>
        <span class="n">y_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">min_root</span><span class="p">,</span> <span class="n">max_root</span><span class="p">,</span> <span class="p">(</span><span class="n">samples_per_degree</span><span class="p">,</span> <span class="n">degree</span><span class="p">))</span>
        <span class="n">y_tmp</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">X_tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">poly</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">y_tmp</span><span class="p">])</span>
        
        <span class="n">root_slice_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">s_</span><span class="p">[</span>
            <span class="n">i</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">,</span>
            <span class="p">:</span><span class="n">degree</span><span class="p">,</span>
            <span class="mi">0</span><span class="p">]</span>
        <span class="n">pad_slice_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">s_</span><span class="p">[</span>
            <span class="n">i</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">,</span>
            <span class="n">degree</span><span class="p">:,</span>
            <span class="mi">1</span><span class="p">]</span>
        <span class="n">this_slice_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">s_</span><span class="p">[</span>
            <span class="n">i</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">samples_per_degree</span><span class="p">,</span>
            <span class="o">-</span><span class="n">degree</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        
        <span class="n">y</span><span class="p">[</span><span class="n">root_slice_y</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_tmp</span> 
        <span class="n">y</span><span class="p">[</span><span class="n">pad_slice_y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">X</span><span class="p">[</span><span class="n">this_slice_X</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_tmp</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">make_this</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">MAX_DEGREE</span><span class="p">,</span> <span class="n">MIN_DEGREE</span><span class="p">,</span> <span class="n">MAX_ROOT</span><span class="p">,</span> <span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">N_SAMPLES</span>
    <span class="k">return</span> <span class="n">make</span><span class="p">(</span><span class="n">N_SAMPLES</span><span class="p">,</span> <span class="n">MAX_DEGREE</span><span class="p">,</span> <span class="n">MIN_DEGREE</span><span class="p">,</span> <span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">MAX_ROOT</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_this</span><span class="p">()</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">'X shapes'</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'y shapes'</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'-'</span><span class="o">*</span><span class="mi">80</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'This is an example root sequence'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>X shapes (110000, 16) (82500, 16) (27500, 16)
y shapes (110000, 15, 2) (82500, 15, 2) (27500, 15, 2)
--------------------------------------------------------------------------------
This is an example root sequence
[[-0.88054536  0.        ]
 [-0.63814698  0.        ]
 [-0.20252743  0.        ]
 [-0.12429279  0.        ]
 [ 0.75713038  0.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]
 [ 0.          1.        ]]
</code></pre>
</div>

<p>Let’s explain this last part;</p>

<ul>
  <li>What you see as the example sequence above representing roots for a polynomial, is a 15x2 matrix.</li>
  <li>Second column has 0s and 1s. When it is a zero, the first column will contain an actual root value.</li>
  <li>When the second column is 1, the first column will have a 0 (as the pad value). This means that row is there as padding.</li>
</ul>

<hr />

<p>Here’s the model. Exactly the same structure, only the dimensions are changed.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">MAX_DEGREE</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">RepeatVector</span><span class="p">(</span><span class="n">MAX_DEGREE</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">LSTM</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">TimeDistributed</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span>

<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_absolute_error'</span><span class="p">,</span>
              <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'mae'</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_3 (LSTM)                (None, 128)               66560     
_________________________________________________________________
repeat_vector_2 (RepeatVecto (None, 15, 128)           0         
_________________________________________________________________
lstm_4 (LSTM)                (None, 15, 128)           131584    
_________________________________________________________________
time_distributed_2 (TimeDist (None, 15, 2)             258       
=================================================================
Total params: 198,402
Trainable params: 198,402
Non-trainable params: 0
_________________________________________________________________
None
</code></pre>
</div>

<p>Here’s a neat trick to quickly see if the layers in the model is set up in a compatible way with the dimensions of the inputs: Before fitting the model, just predict using the test set. If the input tensor flows through it fine, without errors, then the model is good to go.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">));</span>  <span class="c"># this last semi-column will suppress the output.</span>
</code></pre>
</div>

<p>Now let’s fit our model. This time I’ll train it for 10 epochs.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
          <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
          <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
          <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>Train on 82500 samples, validate on 27500 samples
Epoch 1/10
82500/82500 [==============================] - 147s 2ms/step - loss: 0.0473 - mean_absolute_error: 0.0473 - val_loss: 0.0298 - val_mean_absolute_error: 0.0298
Epoch 2/10
82500/82500 [==============================] - 145s 2ms/step - loss: 0.0290 - mean_absolute_error: 0.0290 - val_loss: 0.0255 - val_mean_absolute_error: 0.0255
Epoch 3/10
82500/82500 [==============================] - 138s 2ms/step - loss: 0.0265 - mean_absolute_error: 0.0265 - val_loss: 0.0258 - val_mean_absolute_error: 0.0258
Epoch 4/10
82500/82500 [==============================] - 133s 2ms/step - loss: 0.0248 - mean_absolute_error: 0.0248 - val_loss: 0.0227 - val_mean_absolute_error: 0.0227
Epoch 5/10
82500/82500 [==============================] - 132s 2ms/step - loss: 0.0225 - mean_absolute_error: 0.0225 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202
Epoch 6/10
82500/82500 [==============================] - 131s 2ms/step - loss: 0.0218 - mean_absolute_error: 0.0218 - val_loss: 0.0220 - val_mean_absolute_error: 0.0220
Epoch 7/10
82500/82500 [==============================] - 130s 2ms/step - loss: 0.0203 - mean_absolute_error: 0.0203 - val_loss: 0.0198 - val_mean_absolute_error: 0.0198
Epoch 8/10
82500/82500 [==============================] - 131s 2ms/step - loss: 0.0206 - mean_absolute_error: 0.0206 - val_loss: 0.0191 - val_mean_absolute_error: 0.0191
Epoch 9/10
82500/82500 [==============================] - 130s 2ms/step - loss: 0.0190 - mean_absolute_error: 0.0190 - val_loss: 0.0175 - val_mean_absolute_error: 0.0175
Epoch 10/10
82500/82500 [==============================] - 129s 2ms/step - loss: 0.0185 - mean_absolute_error: 0.0185 - val_loss: 0.0172 - val_mean_absolute_error: 0.0172





&lt;keras.callbacks.History at 0x7f8125726a20&gt;
</code></pre>
</div>

<p>Now there’s one more issue that the varying degree requirement has introduced. Look at what happens when we predict the roots of a polynomial; specifically, check out the second column.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">101</span><span class="p">]))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>array([[[ -3.32482040e-01,   1.54696405e-04],
        [ -5.75829819e-02,  -2.86698341e-05],
        [  1.54504657e-01,   8.33250582e-04],
        [  3.24361920e-01,   1.10552460e-03],
        [  5.12778938e-01,   1.41113997e-05],
        [ -2.66120583e-03,   1.00157547e+00],
        [ -3.35310400e-03,   1.00074172e+00],
        [ -3.01474333e-03,   1.00066710e+00],
        [ -3.09853256e-03,   1.00048482e+00],
        [ -2.91807204e-03,   1.00058520e+00],
        [ -3.08198482e-03,   1.00052881e+00],
        [ -2.97554582e-03,   1.00074744e+00],
        [ -3.03085148e-03,   1.00064242e+00],
        [ -2.96967477e-03,   1.00064325e+00],
        [ -2.97010690e-03,   1.00057983e+00]]], dtype=float32)
</code></pre>
</div>

<p>As we see, the network spits out a bunch of very small numbers, followed by a bunch of numbers that are very close to 1. This is expected behavior for the people familiar with the inner workings of a neural network. However, we’re not interested in this, we want a cutoff threshold and essentially map the second column into 0s and 1s, and we want the root values corresponding to the non-padded rows (i.e. where the second column is 0). To that end, let’s quickly analyze the distribution of the second columns throughout the predictions.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">reshape</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="n">pad_or_not</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'histogram for predicting PAD'</span><span class="p">,</span>
       <span class="n">xlabel</span><span class="o">=</span><span class="s">'predicted value'</span><span class="p">,</span>
       <span class="n">ylabel</span><span class="o">=</span><span class="s">'count'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pad_or_not</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">5</span><span class="p">);</span>
</code></pre>
</div>

<p><img src="img/PolynomialSolver_files/PolynomialSolver_24_0.png" alt="png" /></p>

<p>Looks like the values would be descritized without any issues if we chose the threshold value of 0.5</p>

<hr />

<p>Now I would like to look at the cases where the network did not get the number of roots correctly. The predicted number of roots is going to be determined by the values in the second column higher than the <code class="highlighter-rouge">thr</code> value (which is 0.5)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">thr</span> <span class="o">=</span> <span class="mf">0.5</span>


<span class="k">def</span> <span class="nf">how_many_roots</span><span class="p">(</span><span class="n">predicted</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">thr</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">predicted</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">thr</span><span class="p">)</span>


<span class="n">true_root_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">how_many_roots</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="n">pred_root_count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">how_many_roots</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">Counter</span><span class="p">(</span><span class="n">true_root_count</span> <span class="o">-</span> <span class="n">pred_root_count</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'off by {}: {} times'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">))</span>
</code></pre>
</div>

<div class="highlighter-rouge"><pre class="highlight"><code>off by 0: 27499 times
off by 1: 1 times
</code></pre>
</div>

<p>Fantastic. Only a few mistakes out of 25k predictions [NOTE: this used to be 13 mistakes in a prior run of the notebook lol :)]. To be fair though that was a pretty easy thing to figure out. In fact, it would be interesting to see why those mistakes have occured. Let’s check out how the network fares compared to random prediction. I’m going to have to modify the compare function a little bit since the number of roots vary from case to case.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">true_root_count</span> <span class="o">==</span> <span class="n">pred_root_count</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">predicted_evals</span><span class="p">,</span> <span class="n">random_evals</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="n">random_roots_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">predicted_roots_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">true_roots_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">index</span><span class="p">:</span>
    <span class="n">predicted_roots</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">thr</span><span class="p">]</span>
    <span class="n">true_roots</span> <span class="o">=</span> <span class="p">[</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">random_roots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">MIN_ROOT</span><span class="p">,</span> <span class="n">MAX_ROOT</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">predicted_roots</span><span class="p">))</span>
    <span class="n">random_roots</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">random_roots</span><span class="p">)</span>
    <span class="n">random_roots_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">random_roots</span><span class="p">)</span>
    <span class="n">predicted_roots_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">predicted_roots</span><span class="p">)</span>
    <span class="n">true_roots_list</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">true_roots</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">predicted_root</span><span class="p">,</span> <span class="n">random_root</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">predicted_roots</span><span class="p">,</span> <span class="n">random_roots</span><span class="p">):</span>
        <span class="n">predicted_evals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">predicted_root</span><span class="p">))</span>
        <span class="n">random_evals</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">random_root</span><span class="p">))</span>
        
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_roots_list</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">predicted_roots_list</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">random_roots_list</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">predicted_roots_list</span><span class="p">)</span>
<span class="n">true_roots_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">true_roots_list</span><span class="p">)</span>
<span class="n">random_roots_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">random_roots_list</span><span class="p">)</span>
<span class="n">predicted_roots_list</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predicted_roots_list</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">random_roots_list</span> <span class="o">-</span> <span class="n">true_roots_list</span><span class="p">),</span>
        <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'random guessing'</span><span class="p">)</span> 
<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">predicted_roots_list</span> <span class="o">-</span> <span class="n">true_roots_list</span><span class="p">),</span>
        <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'model predictions'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'Histogram of absolute errors'</span><span class="p">,</span>
       <span class="n">ylabel</span><span class="o">=</span><span class="s">'count'</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s">'absolute error'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">random_evals</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'random guessing kde'</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">predicted_evals</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">kde</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'model prediction kde'</span><span class="p">)</span>
<span class="n">title</span> <span class="o">=</span> <span class="s">'Kernel Density Estimate plot</span><span class="se">\n</span><span class="s">'</span> \
        <span class="s">'for polynomial evaluation of (predicted) roots'</span>
<span class="n">ax</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="o">-.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">5</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre>
</div>

<p><img src="img/PolynomialSolver_files/PolynomialSolver_28_0.png" alt="png" /></p>

<p>Nice.</p>

      </section>
    </div>

    
      <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-106417291-1', 'auto');
        ga('send', 'pageview');
      </script>
    
  </body>
</html>
