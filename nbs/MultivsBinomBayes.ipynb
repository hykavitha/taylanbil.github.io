{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A deep dive to Sklearn's Naive Bayes (NB) algorithms\n",
    "\n",
    "## Intro:\n",
    "\n",
    "Naive Bayes (NB) is a popular algorithm for text-related classification tasks. Sklearn's Multinomial and Bernoulli NB implementations have subtle but quite significant nuances/differences between them. This post will look into these nuances, try to clarify assumptions behind each type of Naive Bayes and try to explain when to use each one.  \n",
    "\n",
    "`sklearn`'s MultinomialNB and BernoulliNB implementations make certain silent assumptions on what kind of datasets they are being trained on. To use these ML algorithms properly, one needs to prepare the training datasets in accordance with these assumptions, which is often overlooked by ML practitioners and this consequently leads to models that are improperly trained.\n",
    "\n",
    "---\n",
    "\n",
    "Let's first see how they behave differently in a very simple example. In the following toy example, our training set is binary, and very small. Let's jump right in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   1  0\n",
       "1   1   0  0\n",
       "2   0   0  1\n",
       "3   0   0  1\n",
       "4   1   1  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, some imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.DataFrame([[0, 1],\n",
    "              [1, 0],\n",
    "              [0, 0],\n",
    "              [0, 0],\n",
    "              [1, 1]], columns=['x1', 'x2'])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "\n",
    "\n",
    "X.join(pd.Series(y, name='y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as you see, we have 5 training samples, with two features. The first two samples belong to class 0, and the other 3 belong to class 1.  \n",
    "\n",
    "Let's fit a `Multinomial` and `Bernoulli` Naive Bayes classifier to this toy dataset. After the models are fit, let's apply it on a sample and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "# no smoothing here, in order to easily understand the computations under the hood\n",
    "mnb = MultinomialNB(alpha=0)  \n",
    "bnb = BernoulliNB(alpha=0)\n",
    "mnb.fit(X, y)\n",
    "bnb.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are fit now. Let's apply it to a sample, where the features `x1` and `x2` are both 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4  0.6]]\n",
      "[[ 0.6  0.4]]\n"
     ]
    }
   ],
   "source": [
    "new_sample = [[1, 1]]\n",
    "\n",
    "\n",
    "print(mnb.predict_proba(new))\n",
    "print(bnb.predict_proba(new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They predict complete opposite probabilities! Ok, what is happening?\n",
    "\n",
    "Let's compute by hand:\n",
    "\n",
    "### **Binomial**:\n",
    "\n",
    "The way we think here is as follows:\n",
    "\n",
    "Each row is a document.  \n",
    "Each feature is a flag, representing __if a word exist in that document or not__.\n",
    "Thus, we end up computing the following conditional probabilities (likelihoods).\n",
    "\n",
    " * P(**a document has x1 in it** | document is 0) = 1/2 $\\quad\\quad$-> Got this from counting the samples where x1 is 1, and class is 0\n",
    " * P(**a document has x2 in it** | document is 0) = 1/2 $\\quad\\quad$-> Similarly, count samples where x1 is 1, and class is 0\n",
    " * P(document is 0) = 2/5 $\\quad\\quad$-> Got this from counting 0s in the y array\n",
    "    \n",
    "So, P(new is 0) ~= 1/2.1/2.2/5 = 1/10. Similarly,  \n",
    "    \n",
    " * P(**a document has x1 in it** | document is 1) = 1/3 $\\quad\\quad$-> Got this from counting the samples where x1 is 1, and class is 1\n",
    " * P(**a document has x2 in it** | document is 1) = 1/3 $\\quad\\quad$-> Similarly, count samples where x1 is 1, and class is 1\n",
    " * P(document is 0) = 3/5 $\\quad\\quad$-> Got this from counting 1s in the y array\n",
    "   \n",
    "So, P(new is 1) ~= 1/3.1/3.3/5 = 1/15  \n",
    "\n",
    "So, P(new=0) is 0.6 and P(new=1) = 0.4  (normalized the probas 1/10 and 1/15 here)\n",
    "\n",
    "---\n",
    "\n",
    "### **Multinomial**:\n",
    "\n",
    "The way we think here is different.\n",
    "\n",
    "Each row is still a document.\n",
    "Each column is now a count, of a word in the documents. This is why MultinomialNB can work with matrices whose entries are non-binary, but positive.\n",
    "\n",
    "Thus, we end up computing the following conditional probabilities (likelihoods).\n",
    "\n",
    " * P(**given word is x1** | document is 0) = 1/2 $\\quad\\quad$-> This is because documents labeled 0 have only 2 words in them, and 1 of them is x1.\n",
    " * P(**given word is x2** | document is 0) = 1/2 $\\quad\\quad$-> Same.\n",
    " * P(document is 0) = 2/5 $\\quad\\quad$-> Got this from counting 0s in the y array\n",
    " \n",
    " So far it is the same, but it differs in below:\n",
    " \n",
    " * P(**a document has x1 in it** | document is 1) = 1/2 $\\quad\\quad$-> __Although there are 3 documents labeled 1, there are 2 words in doc1, and only 1 of them is x1__\n",
    " * P(**a document has x2 in it** | document is 1) = 1/2 $\\quad\\quad$-> Same.\n",
    " * P(document is 0) = 3/5 $\\quad\\quad$-> Got this from counting 1s in the y array\n",
    " \n",
    "Now, work out the probas, and you'll find\n",
    "\n",
    "P(new=0) is 0.4 and P(new=1) = 0.6.\n",
    "\n",
    "---\n",
    "\n",
    "The difference lies in the likelihoods we're computing. In multinomial, we're calculating horizontally (so to speak), the denominator has data from other columns. In binomial, we're calculating vertically only, the denominator does not see any of the other columns.\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANT**: So with these concepts in mind; it should be clear that using `MultinomialNB` with an input matrix containing a negative number does not make sense. In fact, it will just error out if you try to do so. On the other hand, `BernoulliNB` technically works with continuous data too, because it first __binarizes it (with default threshold 0)__. Trying these algorithms on a general dataset without taking measures against these things is will lead to meaningless models in practice.\n",
    "\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "`sklearn`'s `MultinomialNB` and `BernoulliNB` are implementations of Naive Bayes that are built with NLP-related tasks in mind, especially `MultinomialNB`. Specifically, they assume the input they receive are coming from a text preprocessor such as `CountVectorizer` or `TfidfVectorizer`. One needs to do a lot of cleanup and preparation in order to use it in a more general setting, taking how these algorithms interpret the input data into consideration. In the next post, I will explore a more general implementation of Naive Bayes which will apply outside the NLP related tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
