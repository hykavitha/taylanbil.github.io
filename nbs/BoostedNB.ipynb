{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting the Naive Bayes algorithm\n",
    "\n",
    "## 0. Intro\n",
    "\n",
    "In the [last post](/vanillanb), I have included a general implementation of the Naive Bayes algorithm. This implementation differed from `sklearn`'s Naive Bayes algorithms as discussed [here](/multinbvsbinomnb).  \n",
    "\n",
    "It is well known that Naive Bayes does well with text related classification tasks. However, it is not routinely successful in other areas. In this post, I will try to boost the Naive Bayes algorithm in order to end up with a stronger algorithm that does well more often and in general.  \n",
    "\n",
    "I organized this post as follows:\n",
    "\n",
    "* First part will import the code from a file named [nb.py](https://github.com/taylanbil/naivebayes/blob/master/nb.py) and it will show the usage.\n",
    "* Second part will try the code on three publicly available datasets that can be found in the UCI public ML dataset archive.\n",
    "* Last part will discuss the implementation by diving deeper into the code.\n",
    "\n",
    "Before we get started, let's have a quick refresher about the idea behind boosting; very roughly, it is the idea to stack the so called weak-classifiers on top of each other, in such a way that the next one learns from the mistakes of the previous ones combined. In this notebook, our weak-learners are Naive Bayes classifiers. Traditionally though, weak learners are decision-stumps, or in other words very shallow decision trees.\n",
    "\n",
    "Disclaimer: Just as in the previous posts, the goal is to get to a working implementation, so the code is sub-optimal.  \n",
    "Let's dive right into it...\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Intro: Baby steps\n",
    "\n",
    "Let's try the boosted NB on the simple, toy \"play tennis\" dataset. This is the same dataset as in the previous [NB post](/vanillanb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>PlayTennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Temperature Humidity    Wind PlayTennis\n",
       "0      Sunny         Hot     High    Weak         No\n",
       "1      Sunny         Hot     High  Strong         No\n",
       "2   Overcast         Hot     High    Weak        Yes\n",
       "3       Rain        Mild     High    Weak        Yes\n",
       "4       Rain        Cool   Normal    Weak        Yes\n",
       "5       Rain        Cool   Normal  Strong         No\n",
       "6   Overcast        Cool   Normal  Strong        Yes\n",
       "7      Sunny        Mild     High    Weak         No\n",
       "8      Sunny        Cool   Normal    Weak        Yes\n",
       "9       Rain        Mild   Normal    Weak        Yes\n",
       "10     Sunny        Mild   Normal  Strong        Yes\n",
       "11  Overcast        Mild     High  Strong        Yes\n",
       "12  Overcast         Hot   Normal    Weak        Yes\n",
       "13      Rain        Mild     High  Strong         No"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Data is from below. Hardcoding it in order to remove dependency\n",
    "# data = pd.read_csv(\n",
    "#     'https://raw.githubusercontent.com/petehunt/c4.5-compiler/master/example/tennis.csv',\n",
    "#     usecols=['outlook', 'temp', 'humidity', 'wind', 'play']\n",
    "# )\n",
    "\n",
    "data = [\n",
    "    ['Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
    "    ['Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
    "    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
    "    ['Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
    "    ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
    "    ['Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
    "    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n",
    "    ['Sunny', 'Mild', 'High', 'Weak', 'No'],\n",
    "    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
    "    ['Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n",
    "    ['Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n",
    "    ['Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n",
    "    ['Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n",
    "    ['Rain', 'Mild', 'High', 'Strong', 'No'],\n",
    "]\n",
    "data = pd.DataFrame(data, columns='Outlook,Temperature,Humidity,Wind,PlayTennis'.split(','))\n",
    "X = data[data.columns[:-1]]\n",
    "y = data.PlayTennis == 'Yes'\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 4 predictor fields, 14 samples and a binary classification problem. Let's get the boosted NB classifier from [nb.py](https://github.com/taylanbil/naivebayes/blob/master/nb.py) and try it out. The class is called `NaiveBayesBoostingClassifier`. The api is again very `sklearn`-like, with methods such as `fit`, `predict`, `predict_proba`, `decision_function` etc. Let's also compare the boosted NB with the vanilla NB, `NaiveBayesClassifier`, which is the same as before.\n",
    "\n",
    "We will have 2 versions of the boosted NB.\n",
    "\n",
    "1. First one with only 1 iteration, that is, 0 boosting iterations. This case is therefore equivalent to vanilla NB.\n",
    "1. Second one with 2 iterations. The first vanilla NB iteration, and 1 boosting iteration on top of that.\n",
    "\n",
    "The goal is to observe the difference in the calculated posterior probabilities. We will also include the `NaiveBayesClassifier`, and observe that it produces the same posteriors as the trivial boosting case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vanilla</th>\n",
       "      <th>boost_fake</th>\n",
       "      <th>boost_once</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.312031</td>\n",
       "      <td>0.312031</td>\n",
       "      <td>0.313928</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.162746</td>\n",
       "      <td>0.162746</td>\n",
       "      <td>0.164083</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.751472</td>\n",
       "      <td>0.751472</td>\n",
       "      <td>0.752014</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.573354</td>\n",
       "      <td>0.573354</td>\n",
       "      <td>0.573302</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875858</td>\n",
       "      <td>0.875858</td>\n",
       "      <td>0.870270</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.751472</td>\n",
       "      <td>0.751472</td>\n",
       "      <td>0.745584</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.918955</td>\n",
       "      <td>0.918955</td>\n",
       "      <td>0.915018</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.430499</td>\n",
       "      <td>0.430499</td>\n",
       "      <td>0.432647</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.798736</td>\n",
       "      <td>0.798736</td>\n",
       "      <td>0.794659</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.854638</td>\n",
       "      <td>0.854638</td>\n",
       "      <td>0.851980</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.586325</td>\n",
       "      <td>0.586325</td>\n",
       "      <td>0.584993</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.683522</td>\n",
       "      <td>0.683522</td>\n",
       "      <td>0.684268</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.929719</td>\n",
       "      <td>0.929719</td>\n",
       "      <td>0.928607</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.365459</td>\n",
       "      <td>0.365459</td>\n",
       "      <td>0.365355</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     vanilla  boost_fake  boost_once  truth\n",
       "0   0.312031    0.312031    0.313928  False\n",
       "1   0.162746    0.162746    0.164083  False\n",
       "2   0.751472    0.751472    0.752014   True\n",
       "3   0.573354    0.573354    0.573302   True\n",
       "4   0.875858    0.875858    0.870270   True\n",
       "5   0.751472    0.751472    0.745584  False\n",
       "6   0.918955    0.918955    0.915018   True\n",
       "7   0.430499    0.430499    0.432647  False\n",
       "8   0.798736    0.798736    0.794659   True\n",
       "9   0.854638    0.854638    0.851980   True\n",
       "10  0.586325    0.586325    0.584993   True\n",
       "11  0.683522    0.683522    0.684268   True\n",
       "12  0.929719    0.929719    0.928607   True\n",
       "13  0.365459    0.365459    0.365355  False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run nb.py\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "# 1 boosting iteration means, no boosting, and this should spit out exactly the same probas.\n",
    "fake_boosting = NaiveBayesBoostingClassifier(n_iter=1)  \n",
    "# the following is 2 boosting iterations, so the posteriors should differ a bit;\n",
    "real_boosting = NaiveBayesBoostingClassifier(n_iter=2)\n",
    "fake_boosting.fit(X, y)\n",
    "real_boosting.fit(X, y)\n",
    "nb.fit(X, y)\n",
    "\n",
    "out = pd.Series(nb.predict_proba(X)[1], name='vanilla')\n",
    "out = pd.DataFrame(out)\n",
    "cols = ['vanilla', 'boost_fake', 'boost_once', 'truth']\n",
    "out['boost_once'] = real_boosting.decision_function(X)\n",
    "out['boost_fake'] = fake_boosting.decision_function(X)\n",
    "out['truth'] = y\n",
    "out = out[cols]\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that boosting with 1 iteration, i.e. the trivial, non-boosting, produces the same posterior probability as the vanilla NB. On the other hand, the first non-trivial boosting differs from them a tiny bit. It has been changed according to the gradient of the vanilla NB.\n",
    "\n",
    "Now let's apply the boostied NB to some of the publicly available ML datasets. Note that the current version of the boosted NB is __binary classification only__.\n",
    "\n",
    "### 1.b Framework for comparing the performance\n",
    "\n",
    "I'll start this subsection by importing the necessary tools and defining functions which will make it easy to compare the performance of several ML classification algorithms, including `NaiveBayesClassifier` and `NaiveBayesBoostingClassifier`. The other ones are:\n",
    "\n",
    "* `GaussianNB`\n",
    "* `AdaBoost`\n",
    "* `GradientBoostingClassifier` from `sklearn`, which is gradient boosted decision trees.\n",
    "\n",
    "The classifiers listed above are included with their default choice of parameters, without any parameter tuning, as that is beyond the scope of this post. On the other hand, I included two versions of the boosted NB, one with 10 boosting iterations and one with 20. This will help us have a feeling about how the classifier progresses with more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "def make_pipe(est):\n",
    "    pipe = [('imputer', Imputer()),\n",
    "            ('estimator', est)]\n",
    "    return Pipeline(pipe)\n",
    "\n",
    "def make_boost_pipe(n):\n",
    "    pipe_boost = [('preprocessor', NaiveBayesPreprocessor(bins=20)),\n",
    "                  ('nbb', NaiveBayesBoostingClassifier(n_iter=n))]\n",
    "    pipe_boost = Pipeline(pipe_boost)\n",
    "    return pipe_boost\n",
    "\n",
    "%run nb.py\n",
    "pipe = [('preprocessor', NaiveBayesPreprocessor(bins=20)),\n",
    "        ('nb', NaiveBayesClassifier())]\n",
    "pipe = Pipeline(pipe)\n",
    "algos = {'nbayes': pipe,\n",
    "        'gnb': make_pipe(GaussianNB()),\n",
    "        'ada': make_pipe(AdaBoostClassifier()),\n",
    "        'gbt': make_pipe(GradientBoostingClassifier())}\n",
    "for i in [10, 20]:\n",
    "    algos['nbayes+gboost {}'.format(i)] = make_boost_pipe(i)\n",
    "\n",
    "\n",
    "def compare(X, y):\n",
    "    rs = ShuffleSplit(test_size=0.25, n_splits=3)\n",
    "    accuracies = defaultdict(list)\n",
    "    rocs = defaultdict(list)\n",
    "    times = {}\n",
    "    for train_index, test_index in rs.split(X):\n",
    "        X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        for key, est in algos.items():\n",
    "            then = datetime.now()\n",
    "            est.fit(X_train, y_train)\n",
    "            accuracies[key].append(est.score(X_test, y_test))\n",
    "            # now the roc auc\n",
    "            if not isinstance(est.steps[-1][-1], GaussianNB):\n",
    "                ys = est.decision_function(X_test)\n",
    "                if len(ys.shape) == 2 and ys.shape[1] == 2:\n",
    "                    ys = ys[1]\n",
    "            else:\n",
    "                ys = est.predict_proba(X_test)[:, 1]\n",
    "            rocs[key].append(roc_auc_score(y_test, ys))\n",
    "            times[key] = datetime.now() - then\n",
    "    accuracies = pd.DataFrame(accuracies)\n",
    "    rocs = pd.DataFrame(rocs)\n",
    "    times = pd.Series(times)\n",
    "    accuracies.index.name = 'accuracy'\n",
    "    rocs.index.name = 'roc-auc'\n",
    "    times.index.name = 'time'\n",
    "    return accuracies, rocs, times\n",
    "\n",
    "\n",
    "def go(X, y):\n",
    "    accuracies, rocs, times = compare(X, y)\n",
    "    print(accuracies.to_string())\n",
    "    print('-'*80)\n",
    "    print(rocs.to_string())\n",
    "    print('-'*80)\n",
    "    print(times.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at the last line of the `compare` function tells us that it returns the accuracies, roc-auc's and the times it took for all the algorithms it tried. The variable `algos` contains the algorithms we are trying. Now we are in a good position to apply these algorithms to various binary classification problems and see what happens.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Trying it out\n",
    "\n",
    "In this section, the code cells contain urls which refer to the datasets being used. For more information about them, you can follow those links.\n",
    "\n",
    "### 2.1 [Spamdata](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION)\n",
    "\n",
    "The first choice is the spam/non-spam binary classification for emails. A problem that is well known to be a good use case for classical Naive Bayes approach, although that sort of implies the dataset has *bag-of-words* type features and the NB algorithm in question is either `MultinomialNB` or `BernoulliNB`.\n",
    "\n",
    "Let's load the dataset, create our `X` and `y`, and take a quick glance at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2788\n",
      "1    1813\n",
      "Name: 57, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 57 columns):\n",
      "col0     4601 non-null float64\n",
      "col1     4601 non-null float64\n",
      "col2     4601 non-null float64\n",
      "col3     4601 non-null float64\n",
      "col4     4601 non-null float64\n",
      "col5     4601 non-null float64\n",
      "col6     4601 non-null float64\n",
      "col7     4601 non-null float64\n",
      "col8     4601 non-null float64\n",
      "col9     4601 non-null float64\n",
      "col10    4601 non-null float64\n",
      "col11    4601 non-null float64\n",
      "col12    4601 non-null float64\n",
      "col13    4601 non-null float64\n",
      "col14    4601 non-null float64\n",
      "col15    4601 non-null float64\n",
      "col16    4601 non-null float64\n",
      "col17    4601 non-null float64\n",
      "col18    4601 non-null float64\n",
      "col19    4601 non-null float64\n",
      "col20    4601 non-null float64\n",
      "col21    4601 non-null float64\n",
      "col22    4601 non-null float64\n",
      "col23    4601 non-null float64\n",
      "col24    4601 non-null float64\n",
      "col25    4601 non-null float64\n",
      "col26    4601 non-null float64\n",
      "col27    4601 non-null float64\n",
      "col28    4601 non-null float64\n",
      "col29    4601 non-null float64\n",
      "col30    4601 non-null float64\n",
      "col31    4601 non-null float64\n",
      "col32    4601 non-null float64\n",
      "col33    4601 non-null float64\n",
      "col34    4601 non-null float64\n",
      "col35    4601 non-null float64\n",
      "col36    4601 non-null float64\n",
      "col37    4601 non-null float64\n",
      "col38    4601 non-null float64\n",
      "col39    4601 non-null float64\n",
      "col40    4601 non-null float64\n",
      "col41    4601 non-null float64\n",
      "col42    4601 non-null float64\n",
      "col43    4601 non-null float64\n",
      "col44    4601 non-null float64\n",
      "col45    4601 non-null float64\n",
      "col46    4601 non-null float64\n",
      "col47    4601 non-null float64\n",
      "col48    4601 non-null float64\n",
      "col49    4601 non-null float64\n",
      "col50    4601 non-null float64\n",
      "col51    4601 non-null float64\n",
      "col52    4601 non-null float64\n",
      "col53    4601 non-null float64\n",
      "col54    4601 non-null float64\n",
      "col55    4601 non-null int64\n",
      "col56    4601 non-null int64\n",
      "dtypes: float64(55), int64(2)\n",
      "memory usage: 2.0 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>...</th>\n",
       "      <th>col47</th>\n",
       "      <th>col48</th>\n",
       "      <th>col49</th>\n",
       "      <th>col50</th>\n",
       "      <th>col51</th>\n",
       "      <th>col52</th>\n",
       "      <th>col53</th>\n",
       "      <th>col54</th>\n",
       "      <th>col55</th>\n",
       "      <th>col56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.026</td>\n",
       "      <td>6.500</td>\n",
       "      <td>525</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3351</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.428</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2601</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.769</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      col0  col1  col2  col3  col4  col5  col6  col7  col8  col9  ...    \\\n",
       "1783  0.33  0.84  0.67   0.0  0.67  0.33  0.67   0.0  0.33   0.0  ...     \n",
       "3351  0.00  0.00  0.00   0.0  0.00  0.00  0.00   0.0  0.00   0.0  ...     \n",
       "2601  0.00  0.00  0.00   0.0  0.00  0.00  0.00   0.0  0.00   0.0  ...     \n",
       "\n",
       "      col47  col48  col49  col50  col51  col52  col53  col54  col55  col56  \n",
       "1783    0.0    0.0  0.183  0.000  0.156  0.104  0.026  6.500    525    858  \n",
       "3351    0.0    0.0  0.000  0.751  0.000  0.000  0.000  1.428      4     10  \n",
       "2601    0.0    0.0  0.000  0.000  0.000  0.000  0.000  1.769      8     23  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamdata = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',\n",
    "    header=None)\n",
    "X = spamdata[spamdata.columns[:-1]].rename(columns={i: 'col{}'.format(i) for i in spamdata})\n",
    "y = spamdata[spamdata.columns[-1]]\n",
    "print(y.value_counts())\n",
    "print('-'*80)\n",
    "print(X.info())\n",
    "X.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "accuracy                                                                            \n",
      "0         0.936577  0.946134  0.810599  0.893136          0.919201          0.932233\n",
      "1         0.934839  0.947003  0.820156  0.894005          0.933970          0.943527\n",
      "2         0.942659  0.947871  0.817550  0.905300          0.931364          0.942659\n",
      "--------------------------------------------------------------------------------\n",
      "              ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "roc-auc                                                                            \n",
      "0        0.975604  0.983871  0.939048  0.565013          0.975606          0.978455\n",
      "1        0.977105  0.987682  0.942894  0.545387          0.980848          0.984901\n",
      "2        0.979726  0.988510  0.956382  0.527922          0.981500          0.984293\n",
      "--------------------------------------------------------------------------------\n",
      "time\n",
      "ada                00:00:00.250069\n",
      "gbt                00:00:00.527525\n",
      "gnb                00:00:00.006865\n",
      "nbayes             00:00:04.577352\n",
      "nbayes+gboost 10   00:00:28.502304\n",
      "nbayes+gboost 20   00:00:59.384143\n"
     ]
    }
   ],
   "source": [
    "go(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**:\n",
    "\n",
    "* `GaussianNB` produced around 94% roc auc, which is not a bad number. \n",
    "* Vanilla NB produced a 56% roc auc. That somehow translated to a higher accuracy, which suggests a threshold problem with the `GaussianNB`.\n",
    "* Classical boosting algorithms got to 98% roc auc.\n",
    "* Boosted NB with 10 iterations improved significantly over Vanilla NB, and it achieved 98% roc auc. 20 iterations improved slighlty over 10 iterations, which is not a bad sign.\n",
    "\n",
    "So in this case, boosted NB did very comparatively with AdaBoost and Gradient Boosted DTs. Moreover, we observed a good improvement provided by the boosting iterations over the vanilla NB algorithm. We hope this to be a recurring theme.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 [Bankruptcy Data](http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data)\n",
    "\n",
    "Next up I would like to switch to a more business oriented problem. Here we have the data of Polish companies and the ones that went bankrupt. Let's again load the data and take a quick look. Afterwards, let's apply the algorithms and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    41314\n",
      "1     2091\n",
      "Name: class, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43405 entries, 0 to 43404\n",
      "Data columns (total 64 columns):\n",
      "Attr1     43397 non-null float64\n",
      "Attr2     43397 non-null float64\n",
      "Attr3     43397 non-null float64\n",
      "Attr4     43271 non-null float64\n",
      "Attr5     43316 non-null float64\n",
      "Attr6     43397 non-null float64\n",
      "Attr7     43397 non-null float64\n",
      "Attr8     43311 non-null float64\n",
      "Attr9     43396 non-null float64\n",
      "Attr10    43397 non-null float64\n",
      "Attr11    43361 non-null float64\n",
      "Attr12    43271 non-null float64\n",
      "Attr13    43278 non-null float64\n",
      "Attr14    43397 non-null float64\n",
      "Attr15    43369 non-null float64\n",
      "Attr16    43310 non-null float64\n",
      "Attr17    43311 non-null float64\n",
      "Attr18    43397 non-null float64\n",
      "Attr19    43277 non-null float64\n",
      "Attr20    43278 non-null float64\n",
      "Attr21    37551 non-null float64\n",
      "Attr22    43397 non-null float64\n",
      "Attr23    43278 non-null float64\n",
      "Attr24    42483 non-null float64\n",
      "Attr25    43397 non-null float64\n",
      "Attr26    43310 non-null float64\n",
      "Attr27    40641 non-null float64\n",
      "Attr28    42593 non-null float64\n",
      "Attr29    43397 non-null float64\n",
      "Attr30    43278 non-null float64\n",
      "Attr31    43278 non-null float64\n",
      "Attr32    43037 non-null float64\n",
      "Attr33    43271 non-null float64\n",
      "Attr34    43311 non-null float64\n",
      "Attr35    43397 non-null float64\n",
      "Attr36    43397 non-null float64\n",
      "Attr37    24421 non-null float64\n",
      "Attr38    43397 non-null float64\n",
      "Attr39    43278 non-null float64\n",
      "Attr40    43271 non-null float64\n",
      "Attr41    42651 non-null float64\n",
      "Attr42    43278 non-null float64\n",
      "Attr43    43278 non-null float64\n",
      "Attr44    43278 non-null float64\n",
      "Attr45    41258 non-null float64\n",
      "Attr46    43270 non-null float64\n",
      "Attr47    43108 non-null float64\n",
      "Attr48    43396 non-null float64\n",
      "Attr49    43278 non-null float64\n",
      "Attr50    43311 non-null float64\n",
      "Attr51    43397 non-null float64\n",
      "Attr52    43104 non-null float64\n",
      "Attr53    42593 non-null float64\n",
      "Attr54    42593 non-null float64\n",
      "Attr55    43404 non-null float64\n",
      "Attr56    43278 non-null float64\n",
      "Attr57    43398 non-null float64\n",
      "Attr58    43321 non-null float64\n",
      "Attr59    43398 non-null float64\n",
      "Attr60    41253 non-null float64\n",
      "Attr61    43303 non-null float64\n",
      "Attr62    43278 non-null float64\n",
      "Attr63    43271 non-null float64\n",
      "Attr64    42593 non-null float64\n",
      "dtypes: float64(64)\n",
      "memory usage: 21.2 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr55</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28436</th>\n",
       "      <td>0.027486</td>\n",
       "      <td>0.28592</td>\n",
       "      <td>-0.070966</td>\n",
       "      <td>0.64665</td>\n",
       "      <td>-68.132</td>\n",
       "      <td>0.048296</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>2.22740</td>\n",
       "      <td>1.06920</td>\n",
       "      <td>0.63686</td>\n",
       "      <td>...</td>\n",
       "      <td>-3472.5</td>\n",
       "      <td>0.064732</td>\n",
       "      <td>0.043158</td>\n",
       "      <td>0.93527</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>15.0500</td>\n",
       "      <td>14.057</td>\n",
       "      <td>90.126</td>\n",
       "      <td>4.0499</td>\n",
       "      <td>0.93478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31905</th>\n",
       "      <td>0.148640</td>\n",
       "      <td>0.18727</td>\n",
       "      <td>0.594540</td>\n",
       "      <td>10.38300</td>\n",
       "      <td>353.680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183680</td>\n",
       "      <td>4.33980</td>\n",
       "      <td>0.74969</td>\n",
       "      <td>0.81273</td>\n",
       "      <td>...</td>\n",
       "      <td>3424.4</td>\n",
       "      <td>0.232490</td>\n",
       "      <td>0.182890</td>\n",
       "      <td>0.75900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19.1100</td>\n",
       "      <td>34.970</td>\n",
       "      <td>30.850</td>\n",
       "      <td>11.8320</td>\n",
       "      <td>2.19150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.50511</td>\n",
       "      <td>0.084972</td>\n",
       "      <td>1.17170</td>\n",
       "      <td>-32.080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002360</td>\n",
       "      <td>0.97975</td>\n",
       "      <td>2.94260</td>\n",
       "      <td>0.49489</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-0.004278</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.99921</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.6374</td>\n",
       "      <td>12.808</td>\n",
       "      <td>61.386</td>\n",
       "      <td>5.9459</td>\n",
       "      <td>7.00370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Attr1    Attr2     Attr3     Attr4    Attr5     Attr6     Attr7  \\\n",
       "28436  0.027486  0.28592 -0.070966   0.64665  -68.132  0.048296  0.033511   \n",
       "31905  0.148640  0.18727  0.594540  10.38300  353.680  0.000000  0.183680   \n",
       "11506  0.001574  0.50511  0.084972   1.17170  -32.080  0.000000  0.002360   \n",
       "\n",
       "         Attr8    Attr9   Attr10   ...     Attr55    Attr56    Attr57  \\\n",
       "28436  2.22740  1.06920  0.63686   ...    -3472.5  0.064732  0.043158   \n",
       "31905  4.33980  0.74969  0.81273   ...     3424.4  0.232490  0.182890   \n",
       "11506  0.97975  2.94260  0.49489   ...      108.0 -0.004278  0.003180   \n",
       "\n",
       "        Attr58  Attr59   Attr60  Attr61  Attr62   Attr63   Attr64  \n",
       "28436  0.93527  0.1336  15.0500  14.057  90.126   4.0499  0.93478  \n",
       "31905  0.75900  0.0000  19.1100  34.970  30.850  11.8320  2.19150  \n",
       "11506  0.99921  0.0000   8.6374  12.808  61.386   5.9459  7.00370  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polish bankruptcy data from\n",
    "# http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data\n",
    "\n",
    "from scipy.io import arff\n",
    "\n",
    "\n",
    "def load_arff(fn):\n",
    "    X = arff.loadarff(fn)[0]\n",
    "    X = pd.DataFrame(X).applymap(lambda r: r if r != '?' else None)\n",
    "    # X.dropna(how='any', inplace=True)\n",
    "    y = X.pop('class')\n",
    "    y = (y == b'1').astype(int)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "fns = !ls /home/taylanbil/Downloads/*year.arff\n",
    "_ = [load_arff(fn) for fn in fns]\n",
    "X = pd.concat([X for X, y in _]).reset_index(drop=True)\n",
    "y = pd.concat([y for X, y in _]).reset_index(drop=True)\n",
    "del _\n",
    "print(y.value_counts())\n",
    "print('-'*80)\n",
    "print(X.info())\n",
    "X.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "accuracy                                                                            \n",
      "0         0.954386  0.970052  0.078787  0.727055          0.906008          0.932547\n",
      "1         0.957704  0.971250  0.065334  0.730280          0.911076          0.943421\n",
      "2         0.956782  0.971618  0.067822  0.731755          0.902046          0.946461\n",
      "--------------------------------------------------------------------------------\n",
      "              ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "roc-auc                                                                            \n",
      "0        0.879490  0.912178  0.497303  0.733217          0.760798          0.794332\n",
      "1        0.890159  0.921132  0.503383  0.776884          0.786405          0.830294\n",
      "2        0.888399  0.925583  0.495057  0.758146          0.777545          0.813152\n",
      "--------------------------------------------------------------------------------\n",
      "time\n",
      "ada                00:00:12.529434\n",
      "gbt                00:00:15.847193\n",
      "gnb                00:00:00.074148\n",
      "nbayes             00:00:52.060308\n",
      "nbayes+gboost 10   00:04:27.274454\n",
      "nbayes+gboost 20   00:08:38.005423\n"
     ]
    }
   ],
   "source": [
    "go(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**:\n",
    "\n",
    "* Once again, `GaussianNB` performed the worst among the bunch. Not only that, but this time it provided no predictivity, with 50% roc auc.\n",
    "* Classical boosting algorithms did well, with `AdaBoost` around 89% and `GradientBoostingClassifier` around 92% roc auc.\n",
    "* Vanilla NB had around 75% roc auc.\n",
    "* Boosted NB with 10 iterations -> 78% roc auc. Not a ground breaking improvement but an improvement nonetheless.\n",
    "* 20 iterations got us to 81% roc. This suggests a consistent increase in performance.\n",
    "\n",
    "In this example, we have another case of boosting iterations improving the performance. As known from classical boosting algorithms, number of iterations is a hyperparameter one needs to tune for a given problem. It does not seem that we have hit a ceiling of performance here. However, tuning those parameters is not the goal of this post, so we'll skip that discussion.\n",
    "\n",
    "### 2.3 [Credit Card Defaults Data](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)\n",
    "\n",
    "Continuing with the finance theme, third dataset is the credit card default dataset from consumers in Taiwan. Let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    23364\n",
      "1     6636\n",
      "Name: default payment next month, dtype: int64\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 24 columns):\n",
      "ID           30000 non-null int64\n",
      "LIMIT_BAL    30000 non-null int64\n",
      "SEX          30000 non-null int64\n",
      "EDUCATION    30000 non-null int64\n",
      "MARRIAGE     30000 non-null int64\n",
      "AGE          30000 non-null int64\n",
      "PAY_0        30000 non-null int64\n",
      "PAY_2        30000 non-null int64\n",
      "PAY_3        30000 non-null int64\n",
      "PAY_4        30000 non-null int64\n",
      "PAY_5        30000 non-null int64\n",
      "PAY_6        30000 non-null int64\n",
      "BILL_AMT1    30000 non-null int64\n",
      "BILL_AMT2    30000 non-null int64\n",
      "BILL_AMT3    30000 non-null int64\n",
      "BILL_AMT4    30000 non-null int64\n",
      "BILL_AMT5    30000 non-null int64\n",
      "BILL_AMT6    30000 non-null int64\n",
      "PAY_AMT1     30000 non-null int64\n",
      "PAY_AMT2     30000 non-null int64\n",
      "PAY_AMT3     30000 non-null int64\n",
      "PAY_AMT4     30000 non-null int64\n",
      "PAY_AMT5     30000 non-null int64\n",
      "PAY_AMT6     30000 non-null int64\n",
      "dtypes: int64(24)\n",
      "memory usage: 5.5 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22959</th>\n",
       "      <td>22960</td>\n",
       "      <td>210000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>89173</td>\n",
       "      <td>91489</td>\n",
       "      <td>93463</td>\n",
       "      <td>95021</td>\n",
       "      <td>4600</td>\n",
       "      <td>3789</td>\n",
       "      <td>3811</td>\n",
       "      <td>3465</td>\n",
       "      <td>3186</td>\n",
       "      <td>4389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24953</th>\n",
       "      <td>24954</td>\n",
       "      <td>60000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6733</td>\n",
       "      <td>7662</td>\n",
       "      <td>8529</td>\n",
       "      <td>9884</td>\n",
       "      <td>1500</td>\n",
       "      <td>1300</td>\n",
       "      <td>1200</td>\n",
       "      <td>1000</td>\n",
       "      <td>1500</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10352</th>\n",
       "      <td>10353</td>\n",
       "      <td>360000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1090</td>\n",
       "      <td>780</td>\n",
       "      <td>390</td>\n",
       "      <td>388</td>\n",
       "      <td>554</td>\n",
       "      <td>1096</td>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "      <td>388</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "22959  22960     210000    2          1         2   30      0      0      0   \n",
       "24953  24954      60000    1          2         2   30      0      0      0   \n",
       "10352  10353     360000    1          3         1   58     -1     -1     -1   \n",
       "\n",
       "       PAY_4    ...     BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  \\\n",
       "22959      0    ...         89173      91489      93463      95021      4600   \n",
       "24953      0    ...          6733       7662       8529       9884      1500   \n",
       "10352     -1    ...          1090        780        390        388       554   \n",
       "\n",
       "       PAY_AMT2  PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
       "22959      3789      3811      3465      3186      4389  \n",
       "24953      1300      1200      1000      1500       800  \n",
       "10352      1096       780         0       388       887  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients'\n",
    "bankruptcy = pd.read_excel('/home/taylanbil/Downloads/default of credit card clients.xls', skiprows=1)\n",
    "y = bankruptcy.pop('default payment next month')\n",
    "X = bankruptcy\n",
    "print(y.value_counts())\n",
    "print('-'*80)\n",
    "print(X.info())\n",
    "X.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "accuracy                                                                            \n",
      "0         0.813600  0.815467  0.410800  0.757333          0.788133          0.791867\n",
      "1         0.818933  0.823200  0.365067  0.758667          0.790133          0.795333\n",
      "2         0.814667  0.821867  0.371867  0.760933          0.793467          0.799333\n",
      "--------------------------------------------------------------------------------\n",
      "              ada       gbt       gnb    nbayes  nbayes+gboost 10  nbayes+gboost 20\n",
      "roc-auc                                                                            \n",
      "0        0.762167  0.770514  0.658582  0.483075          0.745817          0.739989\n",
      "1        0.779408  0.783143  0.677352  0.479125          0.762096          0.754629\n",
      "2        0.779661  0.787814  0.659249  0.483573          0.758837          0.750321\n",
      "--------------------------------------------------------------------------------\n",
      "time\n",
      "ada                00:00:01.701464\n",
      "gbt                00:00:02.663667\n",
      "gnb                00:00:00.023219\n",
      "nbayes             00:00:13.379899\n",
      "nbayes+gboost 10   00:02:21.470685\n",
      "nbayes+gboost 20   00:04:33.042267\n"
     ]
    }
   ],
   "source": [
    "go(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**:\n",
    "\n",
    "* Similar as above, `AdaBoost` and `GBT` are the best ones.\n",
    "* `GaussianNB` did ok.\n",
    "* Vanilla NB had atrocious performance. 48% roc auc.\n",
    "* Boosted NB with 10 iterations had 76% roc auc. Pretty good performance.\n",
    "* 20 iterations had slightly worse performance; 75% roc auc.\n",
    "\n",
    "This case illustrates the importance of boosting very well. With 1 iteration, NB has absolutely no predictive value. However, once we start boosting, we get to a pretty good performance, competing with the classical boosting algorithms! That is a striking difference. 20 iterations being worse than 10 suggests that the optimal value is less than 20 (it could be less than 10 too, we did not check that). \n",
    "\n",
    "---\n",
    "\n",
    "All in all, we saw the improvement offered by boosting the basic NB algorithm. Although it did not beat the boosted DTs in these examples, there may be cases where it does. There seems to be value in adding this algorithm to one's ML toolkit.\n",
    "\n",
    "Now let's get down to the nitty-gritty and see how this all works.\n",
    "\n",
    "## 3. Code\n",
    "\n",
    "The code can be found [here](https://github.com/taylanbil/naivebayes/blob/master/nb.py). I will paste bits and pieces from that file and try to explain how it comes together.\n",
    "\n",
    "### 3.1. Review of vanilla NB\n",
    "\n",
    "First of all, the boosted NB builds on the classes introduced in the [previous post](/vanillanb). For convenience, here's the code from that discussion.\n",
    "\n",
    "As a quick reminder; the code contains three major parts;\n",
    "\n",
    "1. `NaiveBayesClassifier`, which implements the NB algorithm.\n",
    "2. `NaiveBayesPreprocessor`, which fits and/or transforms a dataset so that the output is suitable for applying the `NaiveBayesClassifier`.\n",
    "3. `Discretizer`, which takes a continuous `pandas` Series and bins it. It also remembers the bins for future use (typically @ scoring time). This is employed by the `NaiveBayesPreprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from bisect import bisect_right\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Discretizer(object):\n",
    "\n",
    "    def __init__(self, upperlim=20, bottomlim=0, mapping=False):\n",
    "        self.mapping = mapping\n",
    "        self.set_lims(upperlim, bottomlim)\n",
    "\n",
    "    @property\n",
    "    def cutoffs(self):\n",
    "        return [i[0] for i in self.mapping]\n",
    "\n",
    "    def set_lims(self, upperlim, bottomlim):\n",
    "        if not self.mapping:\n",
    "            self.bottomlim = bottomlim\n",
    "            self.upperlim = upperlim\n",
    "        else:\n",
    "            vals = sorted(np.unique(map(itemgetter(1), self.mapping)))\n",
    "            self.bottomlim = vals[0]\n",
    "            self.upperlim = vals[-1]\n",
    "        assert self.bottomlim < self.upperlim\n",
    "\n",
    "    def fit(self, continuous_series, subsample=None):\n",
    "        self.mapping = []\n",
    "        continuous_series = pd.Series(continuous_series).reset_index(drop=True).dropna()\n",
    "        if subsample is not None:\n",
    "            n = len(continuous_series)*subsample if subsample < 1 else subsample\n",
    "            continuous_series = np.random.choice(continuous_series, n, replace=False)\n",
    "        ranked = pd.Series(continuous_series).rank(pct=1, method='average')\n",
    "        ranked *= self.upperlim - self.bottomlim\n",
    "        ranked += self.bottomlim\n",
    "        ranked = ranked.map(round)\n",
    "        nvals = sorted(np.unique(ranked))  # sorted in case numpy changes\n",
    "        for nval in nvals:\n",
    "            cond = ranked == nval\n",
    "            self.mapping.append((continuous_series[cond].min(), int(nval)))\n",
    "\n",
    "    def transform_single(self, val):\n",
    "        if not self.mapping:\n",
    "            raise NotImplementedError('Haven\\'t been fitted yet')\n",
    "        elif pd.isnull(val):\n",
    "            return None\n",
    "        i = bisect_right(self.cutoffs, val) - 1\n",
    "        if i == -1:\n",
    "            return 0\n",
    "        return self.mapping[i][1]\n",
    "\n",
    "    def transform(self, vals):\n",
    "        if isinstance(vals, float):\n",
    "            return self.transform_single(vals)\n",
    "        elif vals is None:\n",
    "            return None\n",
    "        return pd.Series(vals).map(self.transform_single)\n",
    "\n",
    "    def fit_transform(self, vals):\n",
    "        self.fit(vals)\n",
    "        return self.transform(vals)\n",
    "\n",
    "\n",
    "class NaiveBayesPreprocessor(object):\n",
    "    \"\"\"\n",
    "    Don't pass in Nans. fill with keyword.\n",
    "    \"\"\"\n",
    "\n",
    "    OTHER = '____OTHER____'\n",
    "    FILLNA = '____NA____'\n",
    "\n",
    "    def __init__(self, alpha=1.0, min_freq=0.01, bins=20):\n",
    "        self.alpha = alpha  # Laplace smoothing\n",
    "        self.min_freq = min_freq  # drop values occuring less frequently than this\n",
    "        self.bins = bins  # number of bins for continuous fields\n",
    "\n",
    "    def learn_continuous_transf(self, series):\n",
    "        D = Discretizer(upperlim=self.bins)\n",
    "        D.fit(series)\n",
    "        return D\n",
    "\n",
    "    def learn_discrete_transf(self, series):\n",
    "        vcs = series.value_counts(dropna=False, normalize=True)\n",
    "        vcs = vcs[vcs >= self.min_freq]\n",
    "        keep = set(vcs.index)\n",
    "        transf = lambda r: r if r in keep else self.OTHER\n",
    "        return transf\n",
    "\n",
    "    def learn_transf(self, series):\n",
    "        if series.dtype == np.float64:\n",
    "            return self.learn_continuous_transf(series)\n",
    "        else:\n",
    "            return self.learn_discrete_transf(series)\n",
    "\n",
    "    def fit(self, X_orig, y=None):\n",
    "        \"\"\"\n",
    "        Expects pandas series and pandas DataFrame\n",
    "        \"\"\"\n",
    "        # get dtypes\n",
    "        self.dtypes = defaultdict(set)\n",
    "        for fld, dtype in X_orig.dtypes.iteritems():\n",
    "            self.dtypes[dtype].add(fld)\n",
    "\n",
    "        X = X_orig\n",
    "        # X = X_orig.fillna(self.FILLNA)\n",
    "        # get transfs\n",
    "        self.transformations = {\n",
    "            fld: self.learn_transf(series)\n",
    "            for fld, series in X.iteritems()}\n",
    "\n",
    "    def transform(self, X_orig, y=None):\n",
    "        \"\"\"\n",
    "        Expects pandas series and pandas DataFrame\n",
    "        \"\"\"\n",
    "        X = X_orig.copy()\n",
    "        # X = X_orig.fillna(self.FILLNA)\n",
    "        for fld, func in self.transformations.items():\n",
    "            if isinstance(func, Discretizer):\n",
    "                X[fld] = func.transform(X[fld])\n",
    "            else:\n",
    "                X[fld] = X[fld].map(func)\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier(object):\n",
    "\n",
    "    def __init__(self, alpha=1.0, class_priors=None, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        self.class_priors = class_priors\n",
    "\n",
    "    def get_class_log_priors(self, y):\n",
    "        self.classes_ = y.unique()\n",
    "        if self.class_priors is None:\n",
    "            self.class_priors = y.value_counts(normalize=1)\n",
    "        elif isinstance(self.class_priors, str) and self.class_priors == 'equal':\n",
    "            raise NotImplementedError\n",
    "        self.class_log_priors = self.class_priors.map(np.log)\n",
    "\n",
    "    def get_log_likelihoods(self, fld):\n",
    "        table = self.groups[fld].value_counts(dropna=False).unstack(fill_value=0)\n",
    "        table += self.alpha\n",
    "        sums = table.sum(axis=1)\n",
    "        likelihoods = table.apply(lambda r: r/sums, axis=0)\n",
    "        log_likelihoods = likelihoods.applymap(np.log)\n",
    "        return {k if pd.notnull(k) else None: v for k, v in log_likelihoods.items()}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = pd.Series(y)\n",
    "        self.get_class_log_priors(y)\n",
    "        self.groups = X.groupby(y)\n",
    "        self.log_likelihoods = {\n",
    "            fld: self.get_log_likelihoods(fld)\n",
    "            for fld, series in X.iteritems()\n",
    "        }\n",
    "\n",
    "    def get_approx_log_posterior(self, series, class_):\n",
    "        log_posterior = self.class_log_priors[class_]  # prior\n",
    "        for fld, val in series.iteritems():\n",
    "            # there are cases where the `val` is not seen before\n",
    "            # as in having a `nan` in the scoring dataset,\n",
    "            #   but no `nans in the training set\n",
    "            # in those cases, we want to not add anything to the log_posterior\n",
    "\n",
    "            # This is to handle the Nones and np.nans etc.\n",
    "            val = val if pd.notnull(val) else None\n",
    "\n",
    "            if val not in self.log_likelihoods[fld]:\n",
    "                continue\n",
    "            log_posterior += self.log_likelihoods[fld][val][class_]\n",
    "        return log_posterior\n",
    "\n",
    "    def decision_function_series(self, series):\n",
    "        approx_log_posteriors = [\n",
    "            self.get_approx_log_posterior(series, class_)\n",
    "            for class_ in self.classes_]\n",
    "        return pd.Series(approx_log_posteriors, index=self.classes_)\n",
    "\n",
    "    def decision_function_df(self, df):\n",
    "        return df.apply(self.decision_function_series, axis=1)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        returns the log posteriors\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return self.decision_function_df(X)\n",
    "        elif isinstance(X, pd.Series):\n",
    "            return self.decision_function_series(X)\n",
    "        elif isinstance(X, dict):\n",
    "            return self.decision_function_series(pd.Series(X))\n",
    "\n",
    "    def predict_proba(self, X, normalize=True):\n",
    "        \"\"\"\n",
    "        returns the (normalized) posterior probability\n",
    "\n",
    "        normalization is just division by the evidence. doesn't change the argmax.\n",
    "        \"\"\"\n",
    "        log_post = self.decision_function(X)\n",
    "        if isinstance(log_post, pd.Series):\n",
    "            post = log_post.map(np.exp)\n",
    "        elif isinstance(log_post, pd.DataFrame):\n",
    "            post = log_post.applymap(np.exp)\n",
    "        else:\n",
    "            raise NotImplementedError('type of X is \"{}\"'.format(type(X)))\n",
    "        if normalize:\n",
    "            if isinstance(post, pd.Series):\n",
    "                post /= post.sum()\n",
    "            elif isinstance(post, pd.DataFrame):\n",
    "                post = post.div(post.sum(axis=1), axis=0)\n",
    "        return post\n",
    "\n",
    "    def predict(self, X):\n",
    "        probas = self.decision_function(X)\n",
    "        if isinstance(probas, pd.Series):\n",
    "            return np.argmax(probas)\n",
    "        return probas.apply(np.argmax, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(np.array(y) == preds.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Quick Review of boosting\n",
    "\n",
    "The idea of boosting consists of these following major parts;\n",
    "\n",
    "1. A weak (or \"base\") learner which fits to the dataset.\n",
    "2. A loss function which evaluates the predictions and the truth values.\n",
    "3. A new weak learner that is trained on the new \"truth values\", which are adjusted so that the past mistakes are weighed more heavily. The idea is to try to correct the mistakes iteratively.\n",
    "4. A method (usually line search) that will define how to bring the new weak learner that is freshly trained into the mix of the past weak learners.\n",
    "\n",
    "To implement this with the NB classifier as the weak learner, we first need a new version of our NB classifier, that can be trained with arbitrary values as the truth set. But the NB algotihm is naturally a classifier. The classical boosting algorithms as `AdaBoost` and `GBT`'s use \"regression trees\", which are decision trees that output continuous values, as their weak learners. Modifying NB to do that seems a bit awkward. Therefore, I chose to not touch the natural \"classifier\" aspect of NB. Instead, I coded a version of the NB classifier with sample weights. That is, the samples that are gotten wrong by the past weak learners are weighed heavily in the next iteration.  \n",
    "\n",
    "Our loss function will be the same as the `BinomialDeviance` loss function that `GBT`s use in `sklearn`.\n",
    "\n",
    "### 3.3. Code for the Boosted NB\n",
    "\n",
    "First let's introduce the weak learner, i.e. the `WeightedNaiveBayesClassifier`. This inherits from the `NaiveBayesClassifier`, and modifies the key methods to accomodate sample weights. The code differs in the way that it stores the log likelihoods and the class priors. The scoring / predicting methods are the same as they retrieve data from the log likelihoods and class priors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WeightedNaiveBayesClassifier(NaiveBayesClassifier):\n",
    "\n",
    "    def get_class_log_priors(self, y, weights):\n",
    "        self.classes_ = y.unique()\n",
    "        self.class_priors = {\n",
    "            class_: ((y == class_)*weights).mean()\n",
    "            for class_ in self.classes_}\n",
    "        self.class_log_priors = {\n",
    "            class_: np.log(prior)\n",
    "            for class_, prior in self.class_priors.items()}\n",
    "\n",
    "    def get_log_likelihood(self, y, class_, series, val, vals, weights):\n",
    "        # indices of `series` and `y` and `weights` must be the same\n",
    "        # XXX: what to do with alpha? should we smooth it out somehow?\n",
    "        cond = y == class_\n",
    "        num = ((series.loc[cond] == val) * weights.loc[cond]).sum() + self.alpha\n",
    "        denom = (weights.loc[cond]).sum() + len(vals) * self.alpha\n",
    "        return np.log(num/denom)\n",
    "\n",
    "    def get_log_likelihoods(self, series, y, weights):\n",
    "        vals = series.unique()\n",
    "        y_ = y.reset_index(drop=True, inplace=False)\n",
    "        series_ = series.reset_index(drop=True, inplace=False)\n",
    "        weights_ = weights.reset_index(drop=True, inplace=False)\n",
    "\n",
    "        log_likelihoods = {\n",
    "            val: {class_: self.get_log_likelihood(y_, class_, series_, val, vals, weights_)\n",
    "                  for class_ in self.classes_}\n",
    "            for val in vals}\n",
    "        return log_likelihoods\n",
    "\n",
    "    def fit(self, X, y, weights=None):\n",
    "        if weights is None:\n",
    "            return super(WeightedNaiveBayesClassifier, self).fit(X, y)\n",
    "        weights *= len(weights) / weights.sum()\n",
    "        y = pd.Series(y)\n",
    "        self.get_class_log_priors(y, weights)\n",
    "        self.log_likelihoods = {\n",
    "            fld: self.get_log_likelihoods(series, y, weights)\n",
    "            for fld, series in X.iteritems()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready for the final piece, the `NaiveBayesBoostingClassifier`. Here's the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesBoostingClassifier(object):\n",
    "    \"\"\"\n",
    "    For now, this is Binary classification only.\n",
    "    `y` needs to consist of 0 or 1\n",
    "    \"\"\"\n",
    "\n",
    "    TRUTHFLD = '__'\n",
    "\n",
    "    def __init__(self, alpha=1.0, n_iter=5, learning_rate=0.1):\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def loss(self, y, pred, vectorize=False):\n",
    "        \"\"\"\n",
    "        This uses the `BinomialDeviance` loss function.\n",
    "        That means, this implementation of NaiveBayesBoostingClassifier\n",
    "            is for binary classification only. Change this function\n",
    "            to implement multiclass classification\n",
    "\n",
    "        Compute the deviance (= 2 * negative log-likelihood).\n",
    "\n",
    "        note that `pred` here is the actual predicted posterior proba.\n",
    "        `pred` in sklearn is log odds\n",
    "        \"\"\"\n",
    "        logodds = np.log(pred/(1-pred))\n",
    "        # logaddexp(0, v) == log(1.0 + exp(v))\n",
    "        ans = -2.0 * ((y * logodds) - np.logaddexp(0.0, logodds))\n",
    "        return ans if vectorize else np.mean(ans)\n",
    "\n",
    "    def adjust_weights(self, X, y, weights):\n",
    "        if weights is None:\n",
    "            return pd.Series([1]*len(y), index=y.index)\n",
    "            # return -- this errors out bc defers to non weighted nb above\n",
    "\n",
    "        pred = self.predicted_posteriors\n",
    "        return self.loss(y, pred, vectorize=True)\n",
    "\n",
    "    def line_search_helper(self, new_preds, step):\n",
    "        if self.predicted_posteriors is None:\n",
    "            return new_preds\n",
    "        ans = self.predicted_posteriors * (1-step)\n",
    "        ans += step * new_preds\n",
    "        return ans\n",
    "\n",
    "    def line_search(self, new_preds, y):\n",
    "        # TODO: This can be done using scipy.optimize.line_search\n",
    "        # but for now, we'll just try 10 values and pick the best one\n",
    "        if not self.line_search_results:\n",
    "            self.line_search_results = [1]\n",
    "            return 1\n",
    "        steps_to_try = -1 * np.arange(\n",
    "            -self.learning_rate, 0, self.learning_rate/10)\n",
    "        step = min(\n",
    "            steps_to_try,\n",
    "            key=lambda s: self.loss(\n",
    "                y, self.line_search_helper(new_preds, s))\n",
    "        )\n",
    "        self.line_search_results.append(step)\n",
    "        return step\n",
    "\n",
    "    def _predict_proba_1(self, est, X):\n",
    "        # XXX: another place where we assume binary clf\n",
    "        # TODO: need a robust way to get 1\n",
    "        return est.predict_proba(X)[1]\n",
    "\n",
    "    def fit(self, X, y, weights=None):\n",
    "        self.stages = []\n",
    "        self.predicted_posteriors = None\n",
    "        self.line_search_results = []\n",
    "        weights = None\n",
    "        for i in range(self.n_iter):\n",
    "            weights = self.adjust_weights(X, y, weights)\n",
    "            nbc = WeightedNaiveBayesClassifier(alpha=self.alpha)\n",
    "            nbc.fit(X, y, weights)\n",
    "            new_preds = self._predict_proba_1(nbc, X)\n",
    "            self.stages.append(nbc)\n",
    "            self.line_search(new_preds, y)\n",
    "            self.predicted_posteriors = self.line_search_helper(\n",
    "                new_preds, self.line_search_results[-1])\n",
    "\n",
    "    def decision_function_df(self, X, staged=False):\n",
    "        stage_posteriors = [\n",
    "            self._predict_proba_1(est, X) for est in self.stages]\n",
    "        posteriors = 0\n",
    "        if staged:\n",
    "            staged_posteriors = dict()\n",
    "        for i, (sp, step) in enumerate(zip(stage_posteriors, self.line_search_results)):\n",
    "            posteriors = (1-step)*posteriors + step*sp\n",
    "            if staged:\n",
    "                staged_posteriors[i] = posteriors\n",
    "        return posteriors if not staged else pd.DataFrame(staged_posteriors)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return self.decision_function_df(X)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def predict(self, X, thr=0.5):\n",
    "        probas = self.decision_function(X)\n",
    "        return (probas >= thr).astype(int)\n",
    "        # # draft for multiclass approach below\n",
    "        # if isinstance(probas, pd.Series):\n",
    "        #     return np.argmax(probas)\n",
    "        # return probas.apply(np.argmax, axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(np.array(y) == preds.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most interesting part of this code is the `fit` method. Here we see the for loop which builds the NB classifiers for each boosting iteration. The `adjust_weights` method computes the new sample weights according to the loss function, which then is fed to the `WeightedNaiveBayesClassifier` for the next weak learner.\n",
    "\n",
    "The way the loss function and the `adjust_weights` method are coded make them restricted to the binary classification case. One would need to replace binomial deviance loss with multinomial deviance loss and modify the weights adjustment accordingly to accomodate the multiclass case.\n",
    "\n",
    "Then comes the line search part. Although it is possible to do an actual line search here, I opted out for a simple \"try 10 things and pick the best one\" method. Not the best approach for sure.\n",
    "\n",
    "Feel free to contact me (@taylanbil in twitter, linkedin, facebook, etc) for any questions/comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
